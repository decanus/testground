# Design blueprint: ensembles

We want testground to support running a test plan with instances built against
different versions sets. For example, running the DHT test plan with 200
instances split into groups:

- Group A: 50 instances are built against go-libp2p-kad-dht/v0.3.0,
  go-libp2p-swarm/v0.3.0
- Group B: 50 instances are built against go-libp2p-kad-dht/v0.3.0,
  go-libp2p-swarm/v0.4.0
- Group C: 50 instances are built against go-libp2p-kad-dht/v0.4.0,
  go-libp2p-swarm/v0.3.0
- Group D: 50 instances are built against go-libp2p-kad-dht/v0.4.0,
  go-libp2p-swarm/v0.4.0

Testground has been designed to offer the primitives to compose such **test
ensembles**. We coin the term **test ensemble** to refer to a set of test
instances, potentially built against different dependency graphs, potentially
running with different parameters, but orchestrated by the same runner, and
participating under the same *run id*.

Thus, all instances comprising the ensemble have visibility over one another,
yet they are configured differently, and such variance is totally opaque.

As long as the DHT plan can be compiled against all upstream versions of the
dependency graph being tested, scheduling the above ensemble would entail:

1. Building the plan 4 times, passing in the appropriate dependency sets (via
   the CLI, this is accomplished with the `--dep` param) each time. We now have
   4 build artifacts, each with its own ID.
2. Running 50 instances of each build artifact, enlisting them in the same run
   ID, providing total instance count=200. This results in all instances
   attaching to the same sync subtree, and awaiting for 200 instances total to
   appear.

As the test scenario runs, each instance outputs metrics and assets. The
challenge now becomes how do we differentiate the metrics generated by each
dependency graph / test parameter set.

In practice, we want to answer questions like: "how many dials per second did
version X generate?", "how many hops in the network did version Y take?", as
well as the larger question: "do these versions interoperate with one another
well?".

The latter question is answered by the global test result, calculated as part of
[#148](https://github.com/ipfs/testground/issues/148).

The former questions can be answered by filtering on the dependency graph of the
instance. We need to implement
[#147](https://github.com/ipfs/testground/issues/147) as a prerequisite.

Filtering by test parameters is already possible, given that each output line
prints the RunEnv, of which the test parameters is a field.

## User interface

<< define how CLI ensemble commands will work >>